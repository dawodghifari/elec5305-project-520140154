{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7007cadd",
   "metadata": {},
   "source": [
    "# Notebook 03: Baseline Models for Instrument Family Classification\n",
    "\n",
    "## Overview\n",
    "This notebook implements two baseline models to establish performance benchmarks for instrument family classification. Both models use traditional machine learning approaches with different feature representations and are designed to be computationally efficient.\n",
    "\n",
    "## Workflow\n",
    "1. **Data Loading and Setup** — Load train/validation/test splits and configure label mappings\n",
    "2. **MFCC + SVM Baseline** — Extract Mel-Frequency Cepstral Coefficients and train Support Vector Machine classifier\n",
    "3. **Mel-Spectrogram + CNN Baseline** — Train lightweight Convolutional Neural Network on mel-spectrogram features\n",
    "4. **Model Training** — Train both models with appropriate hyperparameters\n",
    "5. **Evaluation and Comparison** — Generate confusion matrices and performance metrics for test set\n",
    "6. **Results Export** — Save model predictions and visualizations to Results directory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e51317d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"dbf8230d-6515-4306-8fc2-c4be281a0ae3\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"dbf8230d-6515-4306-8fc2-c4be281a0ae3\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"dbf8230d-6515-4306-8fc2-c4be281a0ae3\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "\n",
      "Loading data and creating label mappings...\n",
      "Classes: ['keyboards', 'percussion', 'strings', 'voice', 'winds']\n",
      "Train: 685 | Val: 79 | Test: 110\n"
     ]
    }
   ],
   "source": [
    "# --- Imports and configuration ---\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# Bokeh imports\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import row\n",
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, ColorBar, BasicTicker\n",
    "from bokeh.transform import transform\n",
    "from bokeh.palettes import Blues9\n",
    "from bokeh.io import push_notebook\n",
    "\n",
    "# Initialize Bokeh for notebook\n",
    "output_notebook()\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Project configuration\n",
    "PROJECT_ROOT = Path(\"/Users/dghifari/02-University/SEM-2-2025/elec5305-project-520140154\")\n",
    "manifests_dir = PROJECT_ROOT / \"Manifests\"\n",
    "train_csv = manifests_dir / \"train.csv\"\n",
    "val_csv   = manifests_dir / \"val.csv\"\n",
    "test_csv  = manifests_dir / \"test.csv\"\n",
    "\n",
    "# Audio processing parameters\n",
    "FAMILY_COLNAME = \"family_label\"\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_SECONDS = 3\n",
    "TARGET_NUM_SAMPLES = SAMPLE_RATE * DURATION_SECONDS\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "\n",
    "# Feature extraction parameters\n",
    "N_MFCC = 13\n",
    "N_MELS = 64\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "\n",
    "# Data loading and label mappings\n",
    "print(\"\\nLoading data and creating label mappings...\")\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "families = sorted(df_train[FAMILY_COLNAME].unique())\n",
    "family_to_idx = {f:i for i,f in enumerate(families)}\n",
    "idx_to_family = {i:f for f,i in family_to_idx.items()}\n",
    "num_classes = len(family_to_idx)\n",
    "\n",
    "print(f\"Classes: {families}\")\n",
    "print(f\"Train: {len(pd.read_csv(train_csv))} | Val: {len(pd.read_csv(val_csv))} | Test: {len(pd.read_csv(test_csv))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5a4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Audio processing utilities ---\n",
    "\n",
    "class Normalize:\n",
    "    \"\"\"Audio normalization utility\"\"\"\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        x = x / (x.abs().max() + 1e-9)\n",
    "        rms = x.pow(2).mean().sqrt()\n",
    "        if rms > 0:\n",
    "            x = x / (rms + 1e-9) * 0.1\n",
    "        return x\n",
    "\n",
    "class AudioDatasetForFeatures(Dataset):\n",
    "    \"\"\"Dataset for loading raw audio files and extracting features\"\"\"\n",
    "    def __init__(self, csv_path, label_map, feature_type='mfcc'):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.label_map = label_map\n",
    "        self.norm = Normalize()\n",
    "        self.target_length = TARGET_NUM_SAMPLES\n",
    "        self.feature_type = feature_type\n",
    "\n",
    "    def _fix_length(self, wav: torch.Tensor, target_len: int):\n",
    "        T = wav.shape[-1]\n",
    "        if T == target_len:\n",
    "            return wav\n",
    "        if T > target_len:\n",
    "            start = (T - target_len) // 2\n",
    "            return wav[..., start:start + target_len]\n",
    "        pad_len = target_len - T\n",
    "        return torch.nn.functional.pad(wav, (0, pad_len))\n",
    "\n",
    "    def _extract_mfcc(self, wav):\n",
    "        \"\"\"Extract MFCC features with temporal statistics\"\"\"\n",
    "        wav_np = wav.numpy()\n",
    "        mfcc = librosa.feature.mfcc(y=wav_np, sr=SAMPLE_RATE, n_mfcc=N_MFCC, \n",
    "                                   hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        mfcc_std = np.std(mfcc, axis=1)\n",
    "        return np.concatenate([mfcc_mean, mfcc_std])\n",
    "    \n",
    "    def _extract_melspectrogram(self, wav):\n",
    "        \"\"\"Extract mel-spectrogram features\"\"\"\n",
    "        wav_np = wav.numpy()\n",
    "        mel_spec = librosa.feature.melspectrogram(y=wav_np, sr=SAMPLE_RATE, \n",
    "                                                 n_mels=N_MELS, hop_length=HOP_LENGTH, \n",
    "                                                 n_fft=N_FFT)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec_db\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        wav_np, sr = sf.read(row['filepath'], dtype='float32')\n",
    "        \n",
    "        wav = torch.from_numpy(wav_np)\n",
    "        if wav.dim() == 1:\n",
    "            wav = wav.unsqueeze(0)\n",
    "        else:\n",
    "            wav = wav.T\n",
    "        \n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        if sr != SAMPLE_RATE:\n",
    "            import torchaudio.functional as F\n",
    "            wav = F.resample(wav, sr, SAMPLE_RATE)\n",
    "        \n",
    "        wav = self._fix_length(wav, self.target_length)\n",
    "        wav = self.norm(wav)\n",
    "        wav = wav.squeeze(0)\n",
    "        \n",
    "        if self.feature_type == 'mfcc':\n",
    "            features = self._extract_mfcc(wav)\n",
    "        elif self.feature_type == 'mel':\n",
    "            features = self._extract_melspectrogram(wav)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature type: {self.feature_type}\")\n",
    "        \n",
    "        label = self.label_map[row[FAMILY_COLNAME]]\n",
    "        return features, label, row['filepath']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "def extract_features_dataset(csv_path, label_map, feature_type='mfcc'):\n",
    "    \"\"\"Extract features for an entire dataset\"\"\"\n",
    "    dataset = AudioDatasetForFeatures(csv_path, label_map, feature_type)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_filepaths = []\n",
    "    \n",
    "    for features, labels, filepaths in tqdm(loader, desc=f\"Extracting {feature_type}\"):\n",
    "        all_features.append(features.numpy().squeeze())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_filepaths.extend(filepaths)\n",
    "    \n",
    "    all_features = np.array(all_features)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    return all_features, all_labels, all_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9d5fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline 1: MFCC + SVM\n",
      "--------------------------------------------------\n",
      "Extracting MFCC features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting mfcc: 100%|██████████| 685/685 [00:05<00:00, 114.48it/s]\n",
      "Extracting mfcc: 100%|██████████| 79/79 [00:00<00:00, 133.27it/s]\n",
      "Extracting mfcc: 100%|██████████| 110/110 [00:00<00:00, 133.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC features shape: (685, 26)\n",
      "Feature dimension: 26\n",
      "\n",
      "Training SVM classifier...\n",
      "Validation Accuracy: 0.7975\n",
      "Validation Macro F1: 0.6694\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards     0.9167    0.5500    0.6875        20\n",
      "  percussion     0.8235    0.9333    0.8750        15\n",
      "     strings     0.0000    0.0000    0.0000         5\n",
      "       voice     0.8947    1.0000    0.9444        17\n",
      "       winds     0.7500    0.9545    0.8400        22\n",
      "\n",
      "    accuracy                         0.7975        79\n",
      "   macro avg     0.6770    0.6876    0.6694        79\n",
      "weighted avg     0.7898    0.7975    0.7773        79\n",
      "\n",
      "\n",
      "Final test evaluation...\n",
      "Test Accuracy: 0.6636\n",
      "Test Macro F1: 0.6386\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards     0.6429    0.8571    0.7347        21\n",
      "  percussion     0.8333    0.5882    0.6897        17\n",
      "     strings     0.7143    0.2381    0.3571        21\n",
      "       voice     0.8182    0.6667    0.7347        27\n",
      "       winds     0.5366    0.9167    0.6769        24\n",
      "\n",
      "    accuracy                         0.6636       110\n",
      "   macro avg     0.7090    0.6534    0.6386       110\n",
      "weighted avg     0.7058    0.6636    0.6431       110\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df3de4fa-70ee-4a43-813c-ebdc2a6bfa1a\" data-root-id=\"p1012\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"7b16dc7f-3579-4f4f-86ad-ee96351ea132\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"p1069\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"p1070\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1012\",\"attributes\":{\"width\":750,\"x_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1022\",\"attributes\":{\"factors\":[\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\"]}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1023\",\"attributes\":{\"factors\":[\"winds\",\"voice\",\"strings\",\"percussion\",\"keyboards\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1024\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1025\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1015\",\"attributes\":{\"text\":\"SVM Confusion Matrix (Test Set)\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1045\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1008\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1009\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1010\"},\"data\":{\"type\":\"map\",\"entries\":[[\"true_label\",[\"keyboards\",\"keyboards\",\"keyboards\",\"keyboards\",\"keyboards\",\"percussion\",\"percussion\",\"percussion\",\"percussion\",\"percussion\",\"strings\",\"strings\",\"strings\",\"strings\",\"strings\",\"voice\",\"voice\",\"voice\",\"voice\",\"voice\",\"winds\",\"winds\",\"winds\",\"winds\",\"winds\"]],[\"pred_label\",[\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\"]],[\"value\",[0.8571428571428571,0.0,0.0,0.047619047619047616,0.09523809523809523,0.058823529411764705,0.5882352941176471,0.0,0.11764705882352941,0.23529411764705882,0.047619047619047616,0.09523809523809523,0.23809523809523808,0.0,0.6190476190476191,0.2962962962962963,0.0,0.037037037037037035,0.6666666666666666,0.0,0.0,0.0,0.041666666666666664,0.041666666666666664,0.9166666666666666]],[\"count\",[18,0,0,1,2,1,10,0,2,4,1,2,5,0,13,8,0,1,18,0,0,0,1,1,22]],[\"pct_text\",[\"86%\",\"0%\",\"0%\",\"5%\",\"10%\",\"6%\",\"59%\",\"0%\",\"12%\",\"24%\",\"5%\",\"10%\",\"24%\",\"0%\",\"62%\",\"30%\",\"0%\",\"4%\",\"67%\",\"0%\",\"0%\",\"0%\",\"4%\",\"4%\",\"92%\"]],[\"count_text\",[\"(18)\",\"(0)\",\"(0)\",\"(1)\",\"(2)\",\"(1)\",\"(10)\",\"(0)\",\"(2)\",\"(4)\",\"(1)\",\"(2)\",\"(5)\",\"(0)\",\"(13)\",\"(8)\",\"(0)\",\"(1)\",\"(18)\",\"(0)\",\"(0)\",\"(0)\",\"(1)\",\"(1)\",\"(22)\"]],[\"text_color\",[\"white\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"white\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1046\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1047\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1042\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1011\",\"attributes\":{\"palette\":[\"#f7fbff\",\"#deebf7\",\"#c6dbef\",\"#9ecae1\",\"#6baed6\",\"#4292c6\",\"#2171b5\",\"#08519c\",\"#08306b\"],\"low\":0,\"high\":1}}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"id\":\"p1011\"}},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1044\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"id\":\"p1011\"}},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1054\",\"attributes\":{\"data_source\":{\"id\":\"p1008\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1055\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1056\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1051\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1052\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.1},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1053\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.2},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1063\",\"attributes\":{\"data_source\":{\"id\":\"p1008\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1064\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1065\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1060\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1061\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.1},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1062\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.2},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1021\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1036\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"True\",\"@true_label\"],[\"Predicted\",\"@pred_label\"],[\"Accuracy\",\"@value{0.1%}\"],[\"Count\",\"@count\"]],\"sort_by\":null}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1037\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1038\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1031\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1032\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1033\"},\"axis_label\":\"True Label\",\"axis_label_text_font_style\":\"bold\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1034\"},\"axis_line_color\":null,\"major_tick_line_color\":null}}],\"right\":[{\"type\":\"object\",\"name\":\"ColorBar\",\"id\":\"p1067\",\"attributes\":{\"location\":[0,0],\"title\":\"Accuracy\",\"title_text_font_style\":\"bold\",\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1066\",\"attributes\":{\"desired_num_ticks\":10,\"mantissas\":[1,2,5]}},\"major_label_policy\":{\"type\":\"object\",\"name\":\"NoOverlap\",\"id\":\"p1068\"},\"label_standoff\":12,\"color_mapper\":{\"id\":\"p1011\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1026\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1027\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1028\"},\"axis_label\":\"Predicted Label\",\"axis_label_text_font_style\":\"bold\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1029\"},\"axis_line_color\":null,\"major_tick_line_color\":null}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1030\",\"attributes\":{\"axis\":{\"id\":\"p1026\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1035\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1031\"},\"grid_line_color\":null}}]}}]}};\n  const render_items = [{\"docid\":\"7b16dc7f-3579-4f4f-86ad-ee96351ea132\",\"roots\":{\"p1012\":\"df3de4fa-70ee-4a43-813c-ebdc2a6bfa1a\"},\"root_ids\":[\"p1012\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1012"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Baseline 1: MFCC + SVM ---\n",
    "\n",
    "print(\"\\nBaseline 1: MFCC + SVM\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create Results directory if it doesn't exist\n",
    "results_dir = PROJECT_ROOT / \"Results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Extract MFCC features\n",
    "print(\"Extracting MFCC features...\")\n",
    "train_mfcc, train_labels, train_paths = extract_features_dataset(train_csv, family_to_idx, 'mfcc')\n",
    "val_mfcc, val_labels, val_paths = extract_features_dataset(val_csv, family_to_idx, 'mfcc')\n",
    "test_mfcc, test_labels, test_paths = extract_features_dataset(test_csv, family_to_idx, 'mfcc')\n",
    "\n",
    "print(f\"MFCC features shape: {train_mfcc.shape}\")\n",
    "print(f\"Feature dimension: {train_mfcc.shape[1]}\")\n",
    "\n",
    "# Standardize features and train SVM\n",
    "print(\"\\nTraining SVM classifier...\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "train_mfcc_scaled = scaler.fit_transform(train_mfcc)\n",
    "val_mfcc_scaled = scaler.transform(val_mfcc)\n",
    "test_mfcc_scaled = scaler.transform(test_mfcc)\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "svm_classifier = SVC(kernel='rbf', C=10.0, gamma='scale', random_state=42)\n",
    "svm_classifier.fit(train_mfcc_scaled, train_labels)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_pred = svm_classifier.predict(val_mfcc_scaled)\n",
    "val_acc = accuracy_score(val_labels, val_pred)\n",
    "val_f1 = f1_score(val_labels, val_pred, average='macro')\n",
    "\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "print(\"\\nValidation Classification Report:\")\n",
    "print(classification_report(val_labels, val_pred, target_names=families, digits=4))\n",
    "\n",
    "# Final test evaluation\n",
    "print(\"\\nFinal test evaluation...\")\n",
    "test_pred = svm_classifier.predict(test_mfcc_scaled)\n",
    "test_acc = accuracy_score(test_labels, test_pred)\n",
    "test_f1 = f1_score(test_labels, test_pred, average='macro')\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_pred, target_names=families, digits=4))\n",
    "\n",
    "# Confusion matrix with Bokeh\n",
    "cm_normalized = confusion_matrix(test_labels, test_pred, normalize='true')\n",
    "cm_counts = confusion_matrix(test_labels, test_pred)\n",
    "n_classes = len(families)\n",
    "\n",
    "# Prepare data\n",
    "true_labels, pred_labels, values, counts, pct_text, count_text, text_colors = [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        true_labels.append(families[i])\n",
    "        pred_labels.append(families[j])\n",
    "        val = cm_normalized[i, j]\n",
    "        cnt = int(cm_counts[i, j])\n",
    "        values.append(val)\n",
    "        counts.append(cnt)\n",
    "        pct_text.append(f'{val:.0%}')\n",
    "        count_text.append(f'({cnt})')\n",
    "        \n",
    "        # Text color for readability\n",
    "        if val < 0.3:\n",
    "            text_colors.append('#2b2b2b')\n",
    "        elif val > 0.7:\n",
    "            text_colors.append('white')\n",
    "        else:\n",
    "            text_colors.append('#2b2b2b')\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    true_label=true_labels,\n",
    "    pred_label=pred_labels,\n",
    "    value=values,\n",
    "    count=counts,\n",
    "    pct_text=pct_text,\n",
    "    count_text=count_text,\n",
    "    text_color=text_colors\n",
    "))\n",
    "\n",
    "# Color mapper - Blues gradient (light to dark)\n",
    "palette = list(reversed(Blues9))\n",
    "mapper = LinearColorMapper(palette=palette, low=0, high=1)\n",
    "\n",
    "# Create figure\n",
    "p = figure(\n",
    "    title=\"SVM Confusion Matrix (Test Set)\",\n",
    "    x_range=families, \n",
    "    y_range=list(reversed(families)),\n",
    "    width=750, \n",
    "    height=600,\n",
    "    tools=\"hover,save,reset\"\n",
    ")\n",
    "\n",
    "p.rect(x=\"pred_label\", y=\"true_label\", width=1, height=1, source=source,\n",
    "       fill_color=transform('value', mapper), line_color='white', line_width=2)\n",
    "\n",
    "p.text(x='pred_label', y='true_label', text='pct_text', source=source,\n",
    "       text_align='center', text_baseline='middle', text_font_size='14pt',\n",
    "       text_font_style='bold', text_color='text_color', y_offset=6)\n",
    "\n",
    "p.text(x='pred_label', y='true_label', text='count_text', source=source,\n",
    "       text_align='center', text_baseline='middle', text_font_size='10pt',\n",
    "       text_color='text_color', y_offset=-8)\n",
    "\n",
    "# Color bar\n",
    "color_bar = ColorBar(color_mapper=mapper, ticker=BasicTicker(desired_num_ticks=10),\n",
    "                     label_standoff=12, border_line_color=None, location=(0, 0),\n",
    "                     title='Accuracy', title_text_font_style='bold')\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "# Hover\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.tooltips = [(\"True\", \"@true_label\"), (\"Predicted\", \"@pred_label\"), \n",
    "                  (\"Accuracy\", \"@value{0.1%}\"), (\"Count\", \"@count\")]\n",
    "\n",
    "# Styling\n",
    "p.grid.grid_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.xaxis.axis_label = 'Predicted Label'\n",
    "p.yaxis.axis_label = 'True Label'\n",
    "p.xaxis.axis_label_text_font_style = \"bold\"\n",
    "p.yaxis.axis_label_text_font_style = \"bold\"\n",
    "\n",
    "show(p)\n",
    "\n",
    "# Store SVM results\n",
    "svm_results = {\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_f1_macro': test_f1,\n",
    "    'confusion_matrix': cm_counts,\n",
    "    'confusion_matrix_normalized': cm_normalized\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41c5585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline 2: Mel-Spectrogram + CNN\n",
      "--------------------------------------------------\n",
      "Extracting mel-spectrogram features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting mel: 100%|██████████| 685/685 [00:02<00:00, 240.48it/s]\n",
      "Extracting mel: 100%|██████████| 79/79 [00:00<00:00, 205.07it/s]\n",
      "Extracting mel: 100%|██████████| 110/110 [00:00<00:00, 229.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mel-spectrogram features shape: (685, 64, 94)\n",
      "Spectrogram shape: (64, 94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Baseline 2: Mel-Spectrogram + CNN ---\n",
    "\n",
    "print(\"\\nBaseline 2: Mel-Spectrogram + CNN\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Extract mel-spectrogram features\n",
    "print(\"Extracting mel-spectrogram features...\")\n",
    "train_mel, train_labels, train_paths = extract_features_dataset(train_csv, family_to_idx, 'mel')\n",
    "val_mel, val_labels, val_paths = extract_features_dataset(val_csv, family_to_idx, 'mel')\n",
    "test_mel, test_labels, test_paths = extract_features_dataset(test_csv, family_to_idx, 'mel')\n",
    "\n",
    "print(f\"Mel-spectrogram features shape: {train_mel.shape}\")\n",
    "print(f\"Spectrogram shape: {train_mel[0].shape}\")\n",
    "\n",
    "# Dataset class for mel-spectrograms\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    \"\"\"Dataset for mel-spectrogram features\"\"\"\n",
    "    def __init__(self, mel_specs, labels):\n",
    "        self.mel_specs = torch.FloatTensor(mel_specs)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        # Add channel dimension for CNN\n",
    "        self.mel_specs = self.mel_specs.unsqueeze(1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.mel_specs[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2b5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CNN model architecture ---\n",
    "\n",
    "class MelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for mel-spectrogram classification - MPS Compatible\n",
    "    \n",
    "    Key fix: Uses AvgPool2d instead of AdaptiveAvgPool2d to avoid MPS limitation\n",
    "    where input sizes must be divisible by output sizes.\n",
    "    \n",
    "    Input: (batch, 1, 64, 94) mel-spectrogram\n",
    "    After 3 MaxPool2d(2,2): (batch, 128, 8, 11)\n",
    "    After AvgPool2d(2,3): (batch, 128, 4, 3) -> Flatten to (batch, 1536)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=5, input_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Conv block 1\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Conv block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Conv block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        \n",
    "        # MPS COMPATIBLE: Use AvgPool2d instead of AdaptiveAvgPool2d\n",
    "        # Input after conv layers: (batch, 128, 8, 11)\n",
    "        # Output after avg pool: (batch, 128, 4, 3)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=(2, 3))\n",
    "        \n",
    "        # Classifier - Updated for new output size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 * 4 * 3, 256),  # 1536 input features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7764b6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model | Parameters: 487,877\n"
     ]
    }
   ],
   "source": [
    "# --- CNN training setup ---\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = MelSpectrogramDataset(train_mel, train_labels)\n",
    "val_dataset = MelSpectrogramDataset(val_mel, val_labels)\n",
    "test_dataset = MelSpectrogramDataset(test_mel, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "cnn_model = MelCNN(num_classes=num_classes).to(DEVICE)\n",
    "print(f\"CNN Model | Parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6900ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CNN training functions ---\n",
    "\n",
    "def train_cnn_one_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"Train CNN for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for mel_specs, labels in tqdm(loader, leave=False, desc=\"Training\"):\n",
    "        mel_specs, labels = mel_specs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(mel_specs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate_cnn(model, loader):\n",
    "    \"\"\"Evaluate CNN model\"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for mel_specs, labels in tqdm(loader, leave=False, desc=\"Evaluating\"):\n",
    "            mel_specs, labels = mel_specs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(mel_specs)\n",
    "            preds = outputs.argmax(1).cpu().numpy()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = 100. * sum([1 for t, p in zip(y_true, y_pred) if t == p]) / len(y_true)\n",
    "    return f1, acc, classification_report(y_true, y_pred, target_names=families, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090a7df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/25 | Loss: 1.5183 | Train:  38.4% | Val:  50.6% | F1: 0.4365 ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/25 | Loss: 0.4073 | Train:  86.1% | Val:  75.9% | F1: 0.7376 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards     1.0000    0.4500    0.6207        20\n",
      "  percussion     0.7895    1.0000    0.8824        15\n",
      "     strings     0.4444    0.8000    0.5714         5\n",
      "       voice     0.8667    0.7647    0.8125        17\n",
      "       winds     0.6667    0.8182    0.7347        22\n",
      "\n",
      "    accuracy                         0.7468        79\n",
      "   macro avg     0.7535    0.7666    0.7243        79\n",
      "weighted avg     0.8033    0.7468    0.7403        79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 | Loss: 0.2897 | Train:  89.8% | Val:  73.4% | F1: 0.7067 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 | Loss: 0.2371 | Train:  91.1% | Val:  75.9% | F1: 0.7194 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards     1.0000    0.4500    0.6207        20\n",
      "  percussion     0.8333    1.0000    0.9091        15\n",
      "     strings     0.4000    0.8000    0.5333         5\n",
      "       voice     1.0000    0.7647    0.8667        17\n",
      "       winds     0.6552    0.8636    0.7451        22\n",
      "\n",
      "    accuracy                         0.7595        79\n",
      "   macro avg     0.7777    0.7757    0.7350        79\n",
      "weighted avg     0.8344    0.7595    0.7575        79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 | Loss: 0.1662 | Train:  94.0% | Val:  73.4% | F1: 0.6678 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 | Loss: 0.1222 | Train:  95.5% | Val:  75.9% | F1: 0.7105 \n",
      "Best CNN F1: 0.7787\n",
      "Model saved to: /Users/dghifari/02-University/SEM-2-2025/elec5305-project-520140154/Results/model_baseline_cnn.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CNN training loop ---\n",
    "\n",
    "print(\"\\nTraining CNN...\")\n",
    "\n",
    "# Create Results directory if it doesn't exist\n",
    "results_dir = PROJECT_ROOT / \"Results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(cnn_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "best_cnn_f1 = 0\n",
    "train_losses, train_accs, val_f1s, val_accs = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_cnn_one_epoch(cnn_model, train_loader, criterion, optimizer)\n",
    "    val_f1, val_acc, report = evaluate_cnn(cnn_model, val_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_cnn_f1:\n",
    "        best_cnn_f1 = val_f1\n",
    "        torch.save(cnn_model.state_dict(), results_dir / \"model_baseline_cnn.pt\")\n",
    "        checkpoint = '✓'\n",
    "    else:\n",
    "        checkpoint = ''\n",
    "    \n",
    "    if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | Loss: {train_loss:.4f} | Train: {train_acc:5.1f}% | Val: {val_acc:5.1f}% | F1: {val_f1:.4f} {checkpoint}\")\n",
    "    \n",
    "    if epoch % 10 == 9:\n",
    "        print(report)\n",
    "\n",
    "print(f\"Best CNN F1: {best_cnn_f1:.4f}\")\n",
    "print(f\"Model saved to: {results_dir / 'model_baseline_cnn.pt'}\")\n",
    "\n",
    "# Load best model for evaluation\n",
    "cnn_model.load_state_dict(torch.load(results_dir / \"model_baseline_cnn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16306c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6182\n",
      "Test Macro F1: 0.5750\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards     1.0000    0.9524    0.9756        21\n",
      "  percussion     0.3696    1.0000    0.5397        17\n",
      "     strings     0.6250    0.2381    0.3448        21\n",
      "       voice     0.7500    0.1111    0.1935        27\n",
      "       winds     0.7188    0.9583    0.8214        24\n",
      "\n",
      "    accuracy                         0.6182       110\n",
      "   macro avg     0.6927    0.6520    0.5750       110\n",
      "weighted avg     0.7083    0.6182    0.5622       110\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"c2c27628-885b-483d-973d-8db1d87002b8\" data-root-id=\"p1075\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"f06459fa-eb53-4288-acad-aed454de94dd\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"p1132\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"p1133\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1075\",\"attributes\":{\"width\":750,\"x_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1085\",\"attributes\":{\"factors\":[\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\"]}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1086\",\"attributes\":{\"factors\":[\"winds\",\"voice\",\"strings\",\"percussion\",\"keyboards\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1087\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1088\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1078\",\"attributes\":{\"text\":\"CNN Confusion Matrix (Test Set)\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1108\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1071\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1072\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1073\"},\"data\":{\"type\":\"map\",\"entries\":[[\"true_label\",[\"keyboards\",\"keyboards\",\"keyboards\",\"keyboards\",\"keyboards\",\"percussion\",\"percussion\",\"percussion\",\"percussion\",\"percussion\",\"strings\",\"strings\",\"strings\",\"strings\",\"strings\",\"voice\",\"voice\",\"voice\",\"voice\",\"voice\",\"winds\",\"winds\",\"winds\",\"winds\",\"winds\"]],[\"pred_label\",[\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\"]],[\"value\",[0.9523809523809523,0.0,0.0,0.0,0.047619047619047616,0.0,1.0,0.0,0.0,0.0,0.0,0.38095238095238093,0.23809523809523808,0.0,0.38095238095238093,0.0,0.7777777777777778,0.1111111111111111,0.1111111111111111,0.0,0.0,0.0,0.0,0.041666666666666664,0.9583333333333334]],[\"count\",[20,0,0,0,1,0,17,0,0,0,0,8,5,0,8,0,21,3,3,0,0,0,0,1,23]],[\"pct_text\",[\"95%\",\"0%\",\"0%\",\"0%\",\"5%\",\"0%\",\"100%\",\"0%\",\"0%\",\"0%\",\"0%\",\"38%\",\"24%\",\"0%\",\"38%\",\"0%\",\"78%\",\"11%\",\"11%\",\"0%\",\"0%\",\"0%\",\"0%\",\"4%\",\"96%\"]],[\"count_text\",[\"(20)\",\"(0)\",\"(0)\",\"(0)\",\"(1)\",\"(0)\",\"(17)\",\"(0)\",\"(0)\",\"(0)\",\"(0)\",\"(8)\",\"(5)\",\"(0)\",\"(8)\",\"(0)\",\"(21)\",\"(3)\",\"(3)\",\"(0)\",\"(0)\",\"(0)\",\"(0)\",\"(1)\",\"(23)\"]],[\"text_color\",[\"white\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"white\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"white\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"white\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1109\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1110\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1105\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1074\",\"attributes\":{\"palette\":[\"#f7fbff\",\"#deebf7\",\"#c6dbef\",\"#9ecae1\",\"#6baed6\",\"#4292c6\",\"#2171b5\",\"#08519c\",\"#08306b\"],\"low\":0,\"high\":1}}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1106\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"id\":\"p1074\"}},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1107\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"id\":\"p1074\"}},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1117\",\"attributes\":{\"data_source\":{\"id\":\"p1071\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1118\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1119\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1114\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1115\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.1},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1116\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.2},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1126\",\"attributes\":{\"data_source\":{\"id\":\"p1071\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1127\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1128\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1123\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1124\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.1},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1125\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.2},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1084\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1099\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"True\",\"@true_label\"],[\"Predicted\",\"@pred_label\"],[\"Accuracy\",\"@value{0.1%}\"],[\"Count\",\"@count\"]],\"sort_by\":null}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1100\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1101\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1094\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1095\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1096\"},\"axis_label\":\"True Label\",\"axis_label_text_font_style\":\"bold\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1097\"},\"axis_line_color\":null,\"major_tick_line_color\":null}}],\"right\":[{\"type\":\"object\",\"name\":\"ColorBar\",\"id\":\"p1130\",\"attributes\":{\"location\":[0,0],\"title\":\"Accuracy\",\"title_text_font_style\":\"bold\",\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1129\",\"attributes\":{\"desired_num_ticks\":10,\"mantissas\":[1,2,5]}},\"major_label_policy\":{\"type\":\"object\",\"name\":\"NoOverlap\",\"id\":\"p1131\"},\"label_standoff\":12,\"color_mapper\":{\"id\":\"p1074\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1089\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1090\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1091\"},\"axis_label\":\"Predicted Label\",\"axis_label_text_font_style\":\"bold\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1092\"},\"axis_line_color\":null,\"major_tick_line_color\":null}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1093\",\"attributes\":{\"axis\":{\"id\":\"p1089\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1098\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1094\"},\"grid_line_color\":null}}]}}]}};\n  const render_items = [{\"docid\":\"f06459fa-eb53-4288-acad-aed454de94dd\",\"roots\":{\"p1075\":\"c2c27628-885b-483d-973d-8db1d87002b8\"},\"root_ids\":[\"p1075\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1075"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CNN final evaluation ---\n",
    "\n",
    "print(\"\\nFinal test evaluation...\")\n",
    "test_f1, test_acc, test_report = evaluate_cnn(cnn_model, test_loader)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc/100:.4f}\")\n",
    "print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(test_report)\n",
    "\n",
    "# Get predictions for confusion matrix\n",
    "cnn_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mel_specs, labels in test_loader:\n",
    "        mel_specs, labels = mel_specs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = cnn_model(mel_specs)\n",
    "        preds = outputs.argmax(1).cpu().numpy()\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "# Confusion matrix with Bokeh\n",
    "cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "cm_counts = confusion_matrix(y_true, y_pred)\n",
    "n_classes = len(families)\n",
    "\n",
    "# Prepare data\n",
    "true_labels, pred_labels, values, counts, pct_text, count_text, text_colors = [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        true_labels.append(families[i])\n",
    "        pred_labels.append(families[j])\n",
    "        val = cm_normalized[i, j]\n",
    "        cnt = int(cm_counts[i, j])\n",
    "        values.append(val)\n",
    "        counts.append(cnt)\n",
    "        pct_text.append(f'{val:.0%}')\n",
    "        count_text.append(f'({cnt})')\n",
    "        \n",
    "        # Text color for readability\n",
    "        if val < 0.3:\n",
    "            text_colors.append('#2b2b2b')\n",
    "        elif val > 0.7:\n",
    "            text_colors.append('white')\n",
    "        else:\n",
    "            text_colors.append('#2b2b2b')\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    true_label=true_labels,\n",
    "    pred_label=pred_labels,\n",
    "    value=values,\n",
    "    count=counts,\n",
    "    pct_text=pct_text,\n",
    "    count_text=count_text,\n",
    "    text_color=text_colors\n",
    "))\n",
    "\n",
    "# Color mapper - Blues gradient (light to dark)\n",
    "palette = list(reversed(Blues9))\n",
    "mapper = LinearColorMapper(palette=palette, low=0, high=1)\n",
    "\n",
    "# Create figure\n",
    "p = figure(\n",
    "    title=\"CNN Confusion Matrix (Test Set)\",\n",
    "    x_range=families, \n",
    "    y_range=list(reversed(families)),\n",
    "    width=750, \n",
    "    height=600,\n",
    "    tools=\"hover,save,reset\"\n",
    ")\n",
    "\n",
    "p.rect(x=\"pred_label\", y=\"true_label\", width=1, height=1, source=source,\n",
    "       fill_color=transform('value', mapper), line_color='white', line_width=2)\n",
    "\n",
    "p.text(x='pred_label', y='true_label', text='pct_text', source=source,\n",
    "       text_align='center', text_baseline='middle', text_font_size='14pt',\n",
    "       text_font_style='bold', text_color='text_color', y_offset=6)\n",
    "\n",
    "p.text(x='pred_label', y='true_label', text='count_text', source=source,\n",
    "       text_align='center', text_baseline='middle', text_font_size='10pt',\n",
    "       text_color='text_color', y_offset=-8)\n",
    "\n",
    "# Color bar\n",
    "color_bar = ColorBar(color_mapper=mapper, ticker=BasicTicker(desired_num_ticks=10),\n",
    "                     label_standoff=12, border_line_color=None, location=(0, 0),\n",
    "                     title='Accuracy', title_text_font_style='bold')\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "# Hover\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.tooltips = [(\"True\", \"@true_label\"), (\"Predicted\", \"@pred_label\"), \n",
    "                  (\"Accuracy\", \"@value{0.1%}\"), (\"Count\", \"@count\")]\n",
    "\n",
    "# Styling\n",
    "p.grid.grid_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.xaxis.axis_label = 'Predicted Label'\n",
    "p.yaxis.axis_label = 'True Label'\n",
    "p.xaxis.axis_label_text_font_style = \"bold\"\n",
    "p.yaxis.axis_label_text_font_style = \"bold\"\n",
    "\n",
    "show(p)\n",
    "\n",
    "# Store CNN results\n",
    "cnn_results = {\n",
    "    'test_accuracy': test_acc/100,\n",
    "    'test_f1_macro': test_f1,\n",
    "    'confusion_matrix': cm_counts,\n",
    "    'confusion_matrix_normalized': cm_normalized\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d422855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Baseline Models\n",
      "--------------------------------------------------\n",
      "MFCC + SVM - Test Accuracy: 0.6636, F1: 0.6386\n",
      "Mel-Spec + CNN - Test Accuracy: 0.6182, F1: 0.5750\n"
     ]
    }
   ],
   "source": [
    "# --- Results summary ---\n",
    "\n",
    "print(\"\\nSummary of Baseline Models\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"MFCC + SVM - Test Accuracy: {svm_results['test_accuracy']:.4f}, F1: {svm_results['test_f1_macro']:.4f}\")\n",
    "print(f\"Mel-Spec + CNN - Test Accuracy: {cnn_results['test_accuracy']:.4f}, F1: {cnn_results['test_f1_macro']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elec5305",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
