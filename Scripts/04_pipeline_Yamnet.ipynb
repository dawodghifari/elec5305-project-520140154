{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23bb755b",
   "metadata": {},
   "source": [
    "# Notebook 04: YAMNet Transfer Learning Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook implements a transfer learning approach using Google's YAMNet pretrained audio model. YAMNet embeddings capture rich acoustic features from AudioSet pretraining, which are then used to train a lightweight classifier for instrument family classification.\n",
    "\n",
    "## Workflow\n",
    "1. **Model and Data Setup** — Load YAMNet model from TensorFlow Hub and configure data splits\n",
    "2. **Embedding Extraction** — Extract 1024-dimensional YAMNet embeddings for all audio clips and cache to disk\n",
    "3. **Classifier Architecture** — Define a simple feedforward classifier on top of frozen YAMNet embeddings\n",
    "4. **Training** — Train classifier with cross-entropy loss and AdamW optimizer\n",
    "5. **Evaluation** — Assess model performance on test set with detailed metrics and confusion matrix\n",
    "6. **Visualization** — Plot training curves and interactive confusion matrix with Bokeh\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338314f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"cd2a1120-728b-4e9f-b364-70d01b115673\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"cd2a1120-728b-4e9f-b364-70d01b115673\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"cd2a1120-728b-4e9f-b364-70d01b115673\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- Imports and configuration ---\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import row\n",
    "from bokeh.models import HoverTool, LinearColorMapper, ColorBar, BasicTicker, ColumnDataSource\n",
    "from bokeh.transform import transform\n",
    "from bokeh.palettes import Blues9\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ROOT = Path(\"/Users/dghifari/02-University/SEM-2-2025/elec5305-project-520140154\")\n",
    "manifests_dir = PROJECT_ROOT / \"Manifests\"\n",
    "train_csv = manifests_dir / \"train.csv\"\n",
    "val_csv   = manifests_dir / \"val.csv\"\n",
    "test_csv  = manifests_dir / \"test.csv\"\n",
    "\n",
    "FAMILY_COLNAME = \"family_label\"\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_SECONDS = 3\n",
    "TARGET_NUM_SAMPLES = SAMPLE_RATE * DURATION_SECONDS\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 25\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb0dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        x = x / (x.abs().max() + 1e-9)\n",
    "        rms = x.pow(2).mean().sqrt()\n",
    "        if rms > 0:\n",
    "            x = x / (rms + 1e-9) * 0.1\n",
    "        return x\n",
    "\n",
    "class AudioDatasetForPreprocessing(Dataset):\n",
    "    \"\"\"Dataset for loading raw audio files\"\"\"\n",
    "    def __init__(self, csv_path, label_map):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.label_map = label_map\n",
    "        self.norm = Normalize()\n",
    "        self.target_length = TARGET_NUM_SAMPLES\n",
    "\n",
    "    def _fix_length(self, wav: torch.Tensor, target_len: int):\n",
    "        T = wav.shape[-1]\n",
    "        if T == target_len:\n",
    "            return wav\n",
    "        if T > target_len:\n",
    "            start = (T - target_len) // 2\n",
    "            return wav[..., start:start + target_len]\n",
    "        pad_len = target_len - T\n",
    "        return torch.nn.functional.pad(wav, (0, pad_len))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        wav_np, sr = sf.read(row['filepath'], dtype='float32')\n",
    "        \n",
    "        wav = torch.from_numpy(wav_np)\n",
    "        if wav.dim() == 1:\n",
    "            wav = wav.unsqueeze(0)\n",
    "        else:\n",
    "            wav = wav.T\n",
    "        \n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        if sr != SAMPLE_RATE:\n",
    "            import torchaudio.functional as F\n",
    "            wav = F.resample(wav, sr, SAMPLE_RATE)\n",
    "        \n",
    "        wav = self._fix_length(wav, self.target_length)\n",
    "        wav = self.norm(wav)\n",
    "        wav = wav.squeeze(0)\n",
    "        \n",
    "        label = self.label_map[row[FAMILY_COLNAME]]\n",
    "        return wav, label, row['filepath']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e82252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['keyboards', 'percussion', 'strings', 'voice', 'winds']\n",
      "Train: 685 | Val: 79 | Test: 110\n"
     ]
    }
   ],
   "source": [
    "# Load metadata and create label mappings\n",
    "df_train = pd.read_csv(train_csv)\n",
    "families = sorted(df_train[FAMILY_COLNAME].unique())\n",
    "family_to_idx = {f:i for i,f in enumerate(families)}\n",
    "idx_to_family = {i:f for f,i in family_to_idx.items()}\n",
    "num_classes = len(family_to_idx)\n",
    "\n",
    "print(f\"Classes: {families}\")\n",
    "print(f\"Train: {len(pd.read_csv(train_csv))} | Val: {len(pd.read_csv(val_csv))} | Test: {len(pd.read_csv(test_csv))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b48c64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached embeddings from Results directory...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130458"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess: Extract YAMNet embeddings and save to disk\n",
    "\n",
    "def extract_yamnet_embeddings_batched(yamnet_model, waveforms_batch):\n",
    "    \"\"\"Extract YAMNet embeddings for a batch of waveforms\"\"\"\n",
    "    embeddings = []\n",
    "    for waveform in waveforms_batch:\n",
    "        with tf.device('/CPU:0'):\n",
    "            _, embeddings_tf, _ = yamnet_model(waveform)\n",
    "            embedding = tf.reduce_mean(embeddings_tf, axis=0)\n",
    "            embeddings.append(embedding.numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def preprocess_and_save_embeddings(split_name, csv_path, yamnet_model, output_path):\n",
    "    \"\"\"Extract and save YAMNet embeddings for a dataset split\"\"\"\n",
    "    dataset = AudioDatasetForPreprocessing(csv_path, family_to_idx)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_filepaths = []\n",
    "    \n",
    "    for waveforms, labels, filepaths in tqdm(loader, desc=f\"{split_name}\"):\n",
    "        waveforms_np = waveforms.numpy().astype(np.float32)\n",
    "        embeddings = extract_yamnet_embeddings_batched(yamnet_model, waveforms_np)\n",
    "        all_embeddings.append(embeddings)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_filepaths.extend(filepaths)\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    np.savez_compressed(output_path, embeddings=all_embeddings, labels=all_labels, filepaths=all_filepaths)\n",
    "    print(f\"Saved {len(all_labels)} embeddings: {all_embeddings.shape}\")\n",
    "\n",
    "# Load YAMNet\n",
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "\n",
    "# Define output paths in Results directory\n",
    "results_dir = PROJECT_ROOT / \"Results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_embeddings_path = results_dir / \"embeddings_yamnet_train.npz\"\n",
    "val_embeddings_path = results_dir / \"embeddings_yamnet_val.npz\"\n",
    "test_embeddings_path = results_dir / \"embeddings_yamnet_test.npz\"\n",
    "\n",
    "# Extract embeddings if not already cached\n",
    "if train_embeddings_path.exists() and val_embeddings_path.exists() and test_embeddings_path.exists():\n",
    "    print(\"Loading cached embeddings from Results directory...\")\n",
    "else:\n",
    "    print(\"Extracting YAMNet embeddings...\")\n",
    "    preprocess_and_save_embeddings(\"train\", train_csv, yamnet_model, train_embeddings_path)\n",
    "    preprocess_and_save_embeddings(\"val\", val_csv, yamnet_model, val_embeddings_path)\n",
    "    preprocess_and_save_embeddings(\"test\", test_csv, yamnet_model, test_embeddings_path)\n",
    "\n",
    "# Clean up\n",
    "del yamnet_model\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b53af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready | Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Dataset for precomputed embeddings\n",
    "\n",
    "class PrecomputedEmbeddingsDataset(Dataset):\n",
    "    \"\"\"Load precomputed YAMNet embeddings\"\"\"\n",
    "    def __init__(self, embeddings_path):\n",
    "        data = np.load(embeddings_path, allow_pickle=True)\n",
    "        self.embeddings = torch.FloatTensor(data['embeddings'])\n",
    "        self.labels = torch.LongTensor(data['labels'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_ds = PrecomputedEmbeddingsDataset(train_embeddings_path)\n",
    "val_ds = PrecomputedEmbeddingsDataset(val_embeddings_path)\n",
    "test_ds = PrecomputedEmbeddingsDataset(test_embeddings_path)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Datasets ready | Batch size: 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c484450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 1024 -> 128 -> 5 | Parameters: 131,845\n"
     ]
    }
   ],
   "source": [
    "# Classifier model\n",
    "\n",
    "class YAMNetClassifier(nn.Module):\n",
    "    \"\"\"Classifier trained on YAMNet embeddings\"\"\"\n",
    "    def __init__(self, embedding_dim=1024, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "model = YAMNetClassifier(embedding_dim=1024, num_classes=num_classes).to(DEVICE)\n",
    "print(f\"Model: 1024 -> 128 -> {num_classes} | Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbd2c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/25 | Loss: 1.5566 | Train:  38.8% | Val:  68.4% | F1: 0.6093 ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/25 | Loss: 1.4187 | Train:  74.0% | Val:  81.0% | F1: 0.6899 ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/25 | Loss: 1.2874 | Train:  82.6% | Val:  82.3% | F1: 0.6864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/25 | Loss: 1.1557 | Train:  85.1% | Val:  86.1% | F1: 0.7162 ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/25 | Loss: 1.0352 | Train:  86.0% | Val:  88.6% | F1: 0.8375 ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/25 | Loss: 0.9257 | Train:  88.5% | Val:  88.6% | F1: 0.8375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards      1.000     0.800     0.889        20\n",
      "  percussion      0.933     0.933     0.933        15\n",
      "     strings      1.000     0.400     0.571         5\n",
      "       voice      0.850     1.000     0.919        17\n",
      "       winds      0.808     0.955     0.875        22\n",
      "\n",
      "    accuracy                          0.886        79\n",
      "   macro avg      0.918     0.818     0.838        79\n",
      "weighted avg      0.902     0.886     0.880        79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/25 | Loss: 0.8116 | Train:  89.8% | Val:  89.9% | F1: 0.8472 ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/25 | Loss: 0.7213 | Train:  89.3% | Val:  89.9% | F1: 0.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/25 | Loss: 0.6426 | Train:  90.8% | Val:  89.9% | F1: 0.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 | Loss: 0.5924 | Train:  91.4% | Val:  89.9% | F1: 0.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 | Loss: 0.5313 | Train:  91.2% | Val:  89.9% | F1: 0.8472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards      1.000     0.800     0.889        20\n",
      "  percussion      0.933     0.933     0.933        15\n",
      "     strings      1.000     0.400     0.571         5\n",
      "       voice      0.895     1.000     0.944        17\n",
      "       winds      0.815     1.000     0.898        22\n",
      "\n",
      "    accuracy                          0.899        79\n",
      "   macro avg      0.929     0.827     0.847        79\n",
      "weighted avg      0.913     0.899     0.892        79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 | Loss: 0.4837 | Train:  91.1% | Val:  91.1% | F1: 0.8595 ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 | Loss: 0.4368 | Train:  91.4% | Val:  89.9% | F1: 0.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 | Loss: 0.4048 | Train:  93.0% | Val:  89.9% | F1: 0.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 | Loss: 0.3801 | Train:  92.6% | Val:  89.9% | F1: 0.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 | Loss: 0.3696 | Train:  92.8% | Val:  91.1% | F1: 0.8509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards      1.000     0.800     0.889        20\n",
      "  percussion      0.938     1.000     0.968        15\n",
      "     strings      0.667     0.400     0.500         5\n",
      "       voice      1.000     1.000     1.000        17\n",
      "       winds      0.815     1.000     0.898        22\n",
      "\n",
      "    accuracy                          0.911        79\n",
      "   macro avg      0.884     0.840     0.851        79\n",
      "weighted avg      0.915     0.911     0.906        79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 | Loss: 0.3431 | Train:  92.4% | Val:  91.1% | F1: 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 | Loss: 0.3275 | Train:  92.8% | Val:  89.9% | F1: 0.8083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 | Loss: 0.3074 | Train:  93.1% | Val:  89.9% | F1: 0.8045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 | Loss: 0.2869 | Train:  94.0% | Val:  87.3% | F1: 0.7370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 | Loss: 0.2958 | Train:  92.4% | Val:  87.3% | F1: 0.7370\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards      1.000     0.800     0.889        20\n",
      "  percussion      0.933     0.933     0.933        15\n",
      "     strings      0.000     0.000     0.000         5\n",
      "       voice      1.000     1.000     1.000        17\n",
      "       winds      0.759     1.000     0.863        22\n",
      "\n",
      "    accuracy                          0.873        79\n",
      "   macro avg      0.738     0.747     0.737        79\n",
      "weighted avg      0.857     0.873     0.858        79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 | Loss: 0.2645 | Train:  93.9% | Val:  88.6% | F1: 0.7439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 | Loss: 0.2542 | Train:  93.7% | Val:  88.6% | F1: 0.7382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 | Loss: 0.2671 | Train:  93.9% | Val:  87.3% | F1: 0.7370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 | Loss: 0.2432 | Train:  94.0% | Val:  88.6% | F1: 0.7439\n",
      "Best F1: 0.8595\n",
      "Model saved to: /Users/dghifari/02-University/SEM-2-2025/elec5305-project-520140154/Results/model_yamnet_classifier.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"abb3a540-8027-4ca3-a090-c567996f1b4d\" data-root-id=\"p1119\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"300d06d6-e150-43a7-876c-b2bcb243ae66\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"p1120\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"p1121\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"p1119\",\"attributes\":{\"children\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1008\",\"attributes\":{\"width\":550,\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1009\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1010\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1018\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1019\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1011\",\"attributes\":{\"text\":\"Accuracy\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1049\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1043\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1044\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1045\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]],[\"y\",[38.832116788321166,74.01459854014598,82.62773722627738,85.1094890510949,85.98540145985402,88.46715328467154,89.78102189781022,89.34306569343066,90.8029197080292,91.38686131386861,91.24087591240875,91.0948905109489,91.38686131386861,92.99270072992701,92.55474452554745,92.84671532846716,92.4087591240876,92.84671532846716,93.13868613138686,94.01459854014598,92.4087591240876,93.86861313868613,93.72262773722628,93.86861313868613,94.01459854014598]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1050\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1051\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1046\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#2E86AB\",\"line_width\":2.5}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1047\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#2E86AB\",\"line_alpha\":0.1,\"line_width\":2.5}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1048\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#2E86AB\",\"line_alpha\":0.2,\"line_width\":2.5}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1060\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1054\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1055\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1056\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]],[\"y\",[68.35443037974683,81.0126582278481,82.27848101265823,86.07594936708861,88.60759493670886,88.60759493670886,89.87341772151899,89.87341772151899,89.87341772151899,89.87341772151899,89.87341772151899,91.13924050632912,89.87341772151899,89.87341772151899,89.87341772151899,91.13924050632912,91.13924050632912,89.87341772151899,89.87341772151899,87.34177215189874,87.34177215189874,88.60759493670886,88.60759493670886,87.34177215189874,88.60759493670886]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1061\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1062\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1057\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#A23B72\",\"line_width\":2.5}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1058\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#A23B72\",\"line_alpha\":0.1,\"line_width\":2.5}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1059\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#A23B72\",\"line_alpha\":0.2,\"line_width\":2.5}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1017\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1030\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1031\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1032\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1033\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1039\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1038\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1040\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1041\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1042\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1025\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1026\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1027\"},\"axis_label\":\"Accuracy (%)\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1028\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1020\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1021\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1022\"},\"axis_label\":\"Epoch\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1023\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1024\",\"attributes\":{\"axis\":{\"id\":\"p1020\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1029\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1025\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1052\",\"attributes\":{\"location\":\"bottom_right\",\"click_policy\":\"hide\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1053\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Train\"},\"renderers\":[{\"id\":\"p1049\"}]}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1063\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Val\"},\"renderers\":[{\"id\":\"p1060\"}]}}]}}]}},{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1064\",\"attributes\":{\"width\":550,\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1065\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1066\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1074\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1075\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1067\",\"attributes\":{\"text\":\"F1 Score\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1105\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1099\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1100\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1101\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]],[\"y\",[0.6093458725182863,0.6899407110705718,0.686394025604552,0.7162282282282282,0.8375139425139425,0.8375139425139425,0.8472108843537415,0.8472108843537415,0.8329251700680272,0.8329251700680272,0.8472108843537415,0.8594894301806745,0.8329251700680272,0.8329251700680272,0.8329251700680272,0.8509180016092458,0.8509180016092458,0.8082785458269329,0.8044690220174091,0.7369934640522875,0.7369934640522875,0.7438751844823951,0.7381608987681094,0.7369934640522875,0.7438751844823951]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1106\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1107\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1102\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#F18F01\",\"line_width\":2.5}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1103\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#F18F01\",\"line_alpha\":0.1,\"line_width\":2.5}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1104\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#F18F01\",\"line_alpha\":0.2,\"line_width\":2.5}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1116\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1110\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1111\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1112\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[12]],[\"y\",[0.8594894301806745]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1117\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1118\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1113\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"},\"hatch_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1114\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1115\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"#C73E1D\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1073\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1086\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1087\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1088\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1089\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1095\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1094\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1096\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1097\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1098\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1081\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1082\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1083\"},\"axis_label\":\"F1\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1084\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1076\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1077\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1078\"},\"axis_label\":\"Epoch\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1079\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1080\",\"attributes\":{\"axis\":{\"id\":\"p1076\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1085\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1081\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1108\",\"attributes\":{\"location\":\"bottom_right\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1109\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Val F1\"},\"renderers\":[{\"id\":\"p1105\"}]}}]}}]}}]}}]}};\n  const render_items = [{\"docid\":\"300d06d6-e150-43a7-876c-b2bcb243ae66\",\"roots\":{\"p1119\":\"abb3a540-8027-4ca3-a090-c567996f1b4d\"},\"root_ids\":[\"p1119\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1119"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# Create Results directory if it doesn't exist\n",
    "results_dir = PROJECT_ROOT / \"Results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for embeddings, labels in tqdm(loader, leave=False, desc=\"Training\"):\n",
    "        embeddings, labels = embeddings.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in tqdm(loader, leave=False, desc=\"Evaluating\"):\n",
    "            embeddings, labels = embeddings.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(embeddings)\n",
    "            preds = outputs.argmax(1).cpu().numpy()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = 100. * sum([1 for t, p in zip(y_true, y_pred) if t == p]) / len(y_true)\n",
    "    return f1, acc, classification_report(y_true, y_pred, target_names=families, digits=3)\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0\n",
    "train_losses, train_accs, val_f1s, val_accs = [], [], [], []\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_f1, val_acc, report = evaluate(model, val_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{EPOCHS} | Loss: {train_loss:.4f} | Train: {train_acc:5.1f}% | Val: {val_acc:5.1f}% | F1: {val_f1:.4f}\", end=\"\")\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), results_dir / \"model_yamnet_classifier.pt\")\n",
    "        print(\" ✓\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        print(report)\n",
    "\n",
    "print(f\"Best F1: {best_f1:.4f}\")\n",
    "print(f\"Model saved to: {results_dir / 'model_yamnet_classifier.pt'}\")\n",
    "\n",
    "# Training curves\n",
    "epochs = list(range(1, EPOCHS + 1))\n",
    "\n",
    "p1 = figure(width=550, height=400, title=\"Accuracy\", x_axis_label=\"Epoch\", y_axis_label=\"Accuracy (%)\")\n",
    "p1.line(epochs, train_accs, line_width=2.5, color='#2E86AB', legend_label='Train')\n",
    "p1.line(epochs, val_accs, line_width=2.5, color='#A23B72', legend_label='Val')\n",
    "p1.legend.location = \"bottom_right\"\n",
    "p1.legend.click_policy = \"hide\"\n",
    "\n",
    "p2 = figure(width=550, height=400, title=\"F1 Score\", x_axis_label=\"Epoch\", y_axis_label=\"F1\")\n",
    "p2.line(epochs, val_f1s, line_width=2.5, color='#F18F01', legend_label='Val F1')\n",
    "best_epoch = val_f1s.index(max(val_f1s)) + 1\n",
    "p2.scatter([best_epoch], [max(val_f1s)], size=12, color='#C73E1D')\n",
    "p2.legend.location = \"bottom_right\"\n",
    "\n",
    "show(row(p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b77db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test F1: 0.8324 | Accuracy: 84.5%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   keyboards      0.952     0.952     0.952        21\n",
      "  percussion      0.708     1.000     0.829        17\n",
      "     strings      1.000     0.476     0.645        21\n",
      "       voice      0.880     0.815     0.846        27\n",
      "       winds      0.800     1.000     0.889        24\n",
      "\n",
      "    accuracy                          0.845       110\n",
      "   macro avg      0.868     0.849     0.832       110\n",
      "weighted avg      0.873     0.845     0.835       110\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"d1ef2241-cda5-4c5b-a6ca-3a44b2ca24f4\" data-root-id=\"p1126\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"6d17d47b-4442-4034-bfc3-1ee17c6c3bf9\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"p1183\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"p1184\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1126\",\"attributes\":{\"width\":750,\"x_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1136\",\"attributes\":{\"factors\":[\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\"]}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1137\",\"attributes\":{\"factors\":[\"winds\",\"voice\",\"strings\",\"percussion\",\"keyboards\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1138\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1139\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1129\",\"attributes\":{\"text\":\"YAMNet Confusion Matrix (Test Set)\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1159\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1122\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1123\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1124\"},\"data\":{\"type\":\"map\",\"entries\":[[\"true_label\",[\"keyboards\",\"keyboards\",\"keyboards\",\"keyboards\",\"keyboards\",\"percussion\",\"percussion\",\"percussion\",\"percussion\",\"percussion\",\"strings\",\"strings\",\"strings\",\"strings\",\"strings\",\"voice\",\"voice\",\"voice\",\"voice\",\"voice\",\"winds\",\"winds\",\"winds\",\"winds\",\"winds\"]],[\"pred_label\",[\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\",\"keyboards\",\"percussion\",\"strings\",\"voice\",\"winds\"]],[\"value\",[0.9523809523809523,0.0,0.0,0.047619047619047616,0.0,0.0,1.0,0.0,0.0,0.0,0.047619047619047616,0.3333333333333333,0.47619047619047616,0.09523809523809523,0.047619047619047616,0.0,0.0,0.0,0.8148148148148148,0.18518518518518517,0.0,0.0,0.0,0.0,1.0]],[\"count\",[20,0,0,1,0,0,17,0,0,0,1,7,10,2,1,0,0,0,22,5,0,0,0,0,24]],[\"pct_text\",[\"95%\",\"0%\",\"0%\",\"5%\",\"0%\",\"0%\",\"100%\",\"0%\",\"0%\",\"0%\",\"5%\",\"33%\",\"48%\",\"10%\",\"5%\",\"0%\",\"0%\",\"0%\",\"81%\",\"19%\",\"0%\",\"0%\",\"0%\",\"0%\",\"100%\"]],[\"count_text\",[\"(20)\",\"(0)\",\"(0)\",\"(1)\",\"(0)\",\"(0)\",\"(17)\",\"(0)\",\"(0)\",\"(0)\",\"(1)\",\"(7)\",\"(10)\",\"(2)\",\"(1)\",\"(0)\",\"(0)\",\"(0)\",\"(22)\",\"(5)\",\"(0)\",\"(0)\",\"(0)\",\"(0)\",\"(24)\"]],[\"text_color\",[\"white\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"white\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"white\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"#2b2b2b\",\"white\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1160\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1161\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1156\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1125\",\"attributes\":{\"palette\":[\"#f7fbff\",\"#deebf7\",\"#c6dbef\",\"#9ecae1\",\"#6baed6\",\"#4292c6\",\"#2171b5\",\"#08519c\",\"#08306b\"],\"low\":0,\"high\":1}}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1157\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"id\":\"p1125\"}},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p1158\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"value\",\"transform\":{\"id\":\"p1125\"}},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1168\",\"attributes\":{\"data_source\":{\"id\":\"p1122\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1169\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1170\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1165\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1166\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.1},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1167\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"pct_text\"},\"y_offset\":{\"type\":\"value\",\"value\":6},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.2},\"text_font_size\":{\"type\":\"value\",\"value\":\"14pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1177\",\"attributes\":{\"data_source\":{\"id\":\"p1122\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1178\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1179\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1174\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1175\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.1},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"p1176\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"pred_label\"},\"y\":{\"type\":\"field\",\"field\":\"true_label\"},\"text\":{\"type\":\"field\",\"field\":\"count_text\"},\"y_offset\":{\"type\":\"value\",\"value\":-8},\"text_color\":{\"type\":\"field\",\"field\":\"text_color\"},\"text_alpha\":{\"type\":\"value\",\"value\":0.2},\"text_font_size\":{\"type\":\"value\",\"value\":\"10pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"},\"text_baseline\":{\"type\":\"value\",\"value\":\"middle\"}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1135\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1150\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"True\",\"@true_label\"],[\"Predicted\",\"@pred_label\"],[\"Accuracy\",\"@value{0.1%}\"],[\"Count\",\"@count\"]],\"sort_by\":null}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1151\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1152\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1145\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1146\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1147\"},\"axis_label\":\"True Label\",\"axis_label_text_font_style\":\"bold\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1148\"},\"axis_line_color\":null,\"major_tick_line_color\":null}}],\"right\":[{\"type\":\"object\",\"name\":\"ColorBar\",\"id\":\"p1181\",\"attributes\":{\"location\":[0,0],\"title\":\"Accuracy\",\"title_text_font_style\":\"bold\",\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1180\",\"attributes\":{\"desired_num_ticks\":10,\"mantissas\":[1,2,5]}},\"major_label_policy\":{\"type\":\"object\",\"name\":\"NoOverlap\",\"id\":\"p1182\"},\"label_standoff\":12,\"color_mapper\":{\"id\":\"p1125\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1140\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1141\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1142\"},\"axis_label\":\"Predicted Label\",\"axis_label_text_font_style\":\"bold\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1143\"},\"axis_line_color\":null,\"major_tick_line_color\":null}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1144\",\"attributes\":{\"axis\":{\"id\":\"p1140\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1149\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1145\"},\"grid_line_color\":null}}]}}]}};\n  const render_items = [{\"docid\":\"6d17d47b-4442-4034-bfc3-1ee17c6c3bf9\",\"roots\":{\"p1126\":\"d1ef2241-cda5-4c5b-a6ca-3a44b2ca24f4\"},\"root_ids\":[\"p1126\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1126"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test evaluation\n",
    "\n",
    "model.load_state_dict(torch.load(results_dir / \"model_yamnet_classifier.pt\", map_location=DEVICE))\n",
    "test_f1, test_acc, test_report = evaluate(model, test_loader)\n",
    "\n",
    "print(f\"\\nTest F1: {test_f1:.4f} | Accuracy: {test_acc:.1f}%\")\n",
    "print(test_report)\n",
    "\n",
    "# Generate predictions\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in test_loader:\n",
    "        embeddings, labels = embeddings.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(embeddings)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(outputs.argmax(1).cpu().numpy())\n",
    "\n",
    "# Confusion matrix\n",
    "cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "cm_counts = confusion_matrix(y_true, y_pred)\n",
    "n_classes = len(families)\n",
    "\n",
    "# Prepare data\n",
    "true_labels, pred_labels, values, counts, pct_text, count_text, text_colors = [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        true_labels.append(families[i])\n",
    "        pred_labels.append(families[j])\n",
    "        val = cm_normalized[i, j]\n",
    "        cnt = int(cm_counts[i, j])\n",
    "        values.append(val)\n",
    "        counts.append(cnt)\n",
    "        pct_text.append(f'{val:.0%}')\n",
    "        count_text.append(f'({cnt})')\n",
    "        \n",
    "        # Text color for readability\n",
    "        if val < 0.3:\n",
    "            text_colors.append('#2b2b2b')\n",
    "        elif val > 0.7:\n",
    "            text_colors.append('white')\n",
    "        else:\n",
    "            text_colors.append('#2b2b2b')\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    true_label=true_labels,\n",
    "    pred_label=pred_labels,\n",
    "    value=values,\n",
    "    count=counts,\n",
    "    pct_text=pct_text,\n",
    "    count_text=count_text,\n",
    "    text_color=text_colors\n",
    "))\n",
    "\n",
    "# Color mapper - Blues gradient (light to dark)\n",
    "palette = list(reversed(Blues9))\n",
    "mapper = LinearColorMapper(palette=palette, low=0, high=1)\n",
    "\n",
    "# Create figure\n",
    "p = figure(\n",
    "    title=\"YAMNet Confusion Matrix (Test Set)\",\n",
    "    x_range=families, \n",
    "    y_range=list(reversed(families)),\n",
    "    width=750, \n",
    "    height=600,\n",
    "    tools=\"hover,save,reset\"\n",
    ")\n",
    "\n",
    "p.rect(x=\"pred_label\", y=\"true_label\", width=1, height=1, source=source,\n",
    "       fill_color=transform('value', mapper), line_color='white', line_width=2)\n",
    "\n",
    "p.text(x='pred_label', y='true_label', text='pct_text', source=source,\n",
    "       text_align='center', text_baseline='middle', text_font_size='14pt',\n",
    "       text_font_style='bold', text_color='text_color', y_offset=6)\n",
    "\n",
    "p.text(x='pred_label', y='true_label', text='count_text', source=source,\n",
    "       text_align='center', text_baseline='middle', text_font_size='10pt',\n",
    "       text_color='text_color', y_offset=-8)\n",
    "\n",
    "# Color bar\n",
    "color_bar = ColorBar(color_mapper=mapper, ticker=BasicTicker(desired_num_ticks=10),\n",
    "                     label_standoff=12, border_line_color=None, location=(0, 0),\n",
    "                     title='Accuracy', title_text_font_style='bold')\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "# Hover\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.tooltips = [(\"True\", \"@true_label\"), (\"Predicted\", \"@pred_label\"), \n",
    "                  (\"Accuracy\", \"@value{0.1%}\"), (\"Count\", \"@count\")]\n",
    "\n",
    "# Styling\n",
    "p.grid.grid_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.xaxis.axis_label = 'Predicted Label'\n",
    "p.yaxis.axis_label = 'True Label'\n",
    "p.xaxis.axis_label_text_font_style = \"bold\"\n",
    "p.yaxis.axis_label_text_font_style = \"bold\"\n",
    "\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elec5305",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
